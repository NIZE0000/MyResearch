{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM with cats dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import nesessary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from modules.dcgan import Discriminator, Generator, initialize_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image size\n",
    "img_size = 64\n",
    "\n",
    "# Set the path to the dataset\n",
    "dataset_path = 'dataset/cats/'\n",
    "\n",
    "# Set the number of images to transform\n",
    "NUM_IMAGES = 15747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of image filenames\n",
    "image_filenames = os.listdir(dataset_path)\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # convert PIL image to tensor and scales data into [0,1] \n",
    "    # transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # Scale between [-1, 1] by (input[channel] - mean[channel]) / std[channel]\n",
    "])\n",
    "\n",
    "# Create a list to store the transformed images\n",
    "transformed_images = []\n",
    "\n",
    "# Iterate over the first num_images filenames and transform the corresponding images\n",
    "for i, filename in enumerate(image_filenames[:NUM_IMAGES]):\n",
    "    # Load the image\n",
    "    img_path = os.path.join(dataset_path, filename)\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    # Apply the transformations\n",
    "    transformed_image = transform(image)\n",
    "\n",
    "    # Append the transformed image to the list\n",
    "    transformed_images.append(transformed_image)\n",
    "\n",
    "# Convert the list of transformed images to a PyTorch tensor\n",
    "transformed_images = torch.stack(transformed_images)\n",
    "\n",
    "print(f'Loaded data: {transformed_images.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set batch size\n",
    "batch_size = 16\n",
    "\n",
    "data_loader = DataLoader(transformed_images, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "print(next(data_iter).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the model\n",
    "\n",
    "base on DDPM and unet papers \n",
    "https://arxiv.org/pdf/1505.04597v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ddpm import Diffusion, initialize_weights\n",
    "from .modules.unet import UNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter before training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base on the paper\n",
    "LEARNING_RATE = 1e-4  #0.0001\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "LATENT_DIM = 100\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and save weight and log "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training process\n",
    "parameter base on DDPM paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import tqdm\n",
    "\n",
    "\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "mse = nn.MSELoss()\n",
    "diffusion = Diffusion(img_size=IMAGE_SIZE, device=device)\n",
    "writer = SummaryWriter(os.path.join(\"logs/cats\",\"DDPM\"))\n",
    "\n",
    "l = len(data_loader)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    logging.info(f\"Starting epoch {epoch}:\")\n",
    "\n",
    "\n",
    "    model.train()\n",
    "    pbar = tqdm(data_loader)\n",
    "    for i, (images, _) in enumerate(pbar):\n",
    "        images = images.to(device)\n",
    "        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "        x_t, noise = diffusion.noise_images(images, t)\n",
    "        predicted_noise = model(x_t, t)\n",
    "        loss = mse(noise, predicted_noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pbar.set_postfix(MSE=loss.item())\n",
    "        writer.add_scalar(\"MSE\", loss.item(), global_step=epoch * l + i)\n",
    "\n",
    "    sampled_images = diffusion.sample(model, n=images.shape[0])\n",
    "    torch.save(model.state_dict(), os.path.join(\"weights\", \"cats\", \"DDPM\", f\"{epoch+1}.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = diffusion.sample(model, 8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure FID\n",
    "\n",
    "use this implementation: https://github.com/mseitzer/pytorch-fid/tree/master\n",
    "\n",
    "You need to import model first  \n",
    "and masure define gen_log function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
