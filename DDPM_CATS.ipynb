{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPM with cats dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import nesessary modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from modules.dcgan import Discriminator, Generator, initialize_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set agnostic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agnostic code\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set image size\n",
    "img_size = 64\n",
    "\n",
    "# Set the path to the dataset\n",
    "dataset_path = 'dataset/cats/'\n",
    "\n",
    "# Set the number of images to transform\n",
    "NUM_IMAGES = 15747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data: torch.Size([15747, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Get the list of image filenames\n",
    "image_filenames = os.listdir(dataset_path)\n",
    "\n",
    "# Define the image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    # transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(), # convert PIL image to tensor and scales data into [0,1] \n",
    "    # transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]), # Scale between [-1, 1] by (input[channel] - mean[channel]) / std[channel]\n",
    "])\n",
    "\n",
    "# Create a list to store the transformed images\n",
    "transformed_images = []\n",
    "\n",
    "# Iterate over the first num_images filenames and transform the corresponding images\n",
    "for i, filename in enumerate(image_filenames[:NUM_IMAGES]):\n",
    "    # Load the image\n",
    "    img_path = os.path.join(dataset_path, filename)\n",
    "    image = Image.open(img_path)\n",
    "\n",
    "    # Apply the transformations\n",
    "    transformed_image = transform(image)\n",
    "\n",
    "    # Append the transformed image to the list\n",
    "    transformed_images.append(transformed_image)\n",
    "\n",
    "# Convert the list of transformed images to a PyTorch tensor\n",
    "transformed_images = torch.stack(transformed_images)\n",
    "\n",
    "print(f'Loaded data: {transformed_images.shape}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# set batch size\n",
    "batch_size = 16\n",
    "\n",
    "data_loader = DataLoader(transformed_images, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "data_iter = iter(data_loader)\n",
    "print(next(data_iter).shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import the model\n",
    "\n",
    "base on DDPM and unet papers \n",
    "https://arxiv.org/pdf/1505.04597v1.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.ddpm import Diffusion\n",
    "\n",
    "from modules.modules import UNet"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set hyperparameter before training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base on the paper\n",
    "LEARNING_RATE = 1e-4  #0.0001\n",
    "BATCH_SIZE = 16\n",
    "IMAGE_SIZE = 64\n",
    "CHANNELS_IMG = 3\n",
    "LATENT_DIM = 100\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model and save weight and log "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training process\n",
    "parameter base on DDPM paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:53:26 - INFO: Starting epoch 0:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] Batch 984/984 Using Time: 255.3108            Loss: 0.0069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "06:57:41 - INFO: Sampling 32 new images....\n",
      "999it [02:46,  6.00it/s]\n",
      "07:00:28 - INFO: Starting epoch 1:\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import time\n",
    "\n",
    "model = UNet().to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "mse = nn.MSELoss()\n",
    "diffusion = Diffusion(img_size=IMAGE_SIZE, device=device)\n",
    "writer = SummaryWriter(os.path.join(\"logs/cats\",\"DDPM\"))\n",
    "\n",
    "l = len(data_loader)\n",
    "\n",
    "time_use = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\n",
    "    # Shuffle the dataset at the beginning of each epoch\n",
    "    data_loader = DataLoader(transformed_images, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    lossMean = 0\n",
    "\n",
    "    # logging.info(f\"Starting epoch {epoch}:\")\n",
    "\n",
    "    # use time for time measurement\n",
    "    start_time = time.time()\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, images in enumerate(data_loader):\n",
    "        images = images.to(device)\n",
    "        t = diffusion.sample_timesteps(images.shape[0]).to(device)\n",
    "        x_t, noise = diffusion.noise_images(images, t)\n",
    "        predicted_noise = model(x_t, t)\n",
    "        loss = mse(noise, predicted_noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # sum lose\n",
    "        lossMean += loss\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "    time_use += epoch_time \n",
    "\n",
    "    epoch_index = epoch+1\n",
    "\n",
    "    # calculate mean value\n",
    "    lossMean = lossMean / len(data_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch [{epoch_index}/{NUM_EPOCHS}] Batch {batch_idx+1}/{len(data_loader)} Using Time: {epoch_time:.4f}\\\n",
    "            Loss: {lossMean:.4f}\"\n",
    "    )\n",
    "\n",
    "    # tensorboard\n",
    "    writer.add_scalar(\"MSE\", lossMean, global_step=epoch+1)\n",
    "    writer.add_scalar(\"traing time\", time_use, epoch_index)\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(\"weights\", \"cats\", \"DDPM\", f\"{epoch_index}.pt\"))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = diffusion.sample(model, n=32).type(dtype=torch.float32)\n",
    "        img_grid = torchvision.utils.make_grid(x[:32], normalize=True)\n",
    "        writer.add_image(\"All/Generate Images\", img_grid, global_step=epoch_index)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measure FID\n",
    "\n",
    "use this implementation: https://github.com/mseitzer/pytorch-fid/tree/master"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define FID measurement function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import torch\n",
    "from torchvision.utils import save_image\n",
    "import re\n",
    "\n",
    "# Create a function to run the FID script\n",
    "def run_fid(real_path, gen_path, epoch):\n",
    "    command = [\"python\", \"-m\", \"pytorch_fid\", real_path, gen_path]\n",
    "\n",
    "    output = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "    # Extract the FID score using regular expressions\n",
    "    output = output.stdout\n",
    "    fid_score_match = re.search(r\"FID:\\s+(\\d+\\.\\d+)\", output)\n",
    "    \n",
    "    if fid_score_match:\n",
    "        fid_score = float(fid_score_match.group(1))\n",
    "        print(\"FID score:\", fid_score)\n",
    "\n",
    "        # Write the FID score to the log file\n",
    "        # gen_log(log_path=\"logs/cats/fid/DCGAN.log\", message=f\"Epoch: {epoch}, FID score: {fid_score}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"FID score not found in the output.\")\n",
    "\n",
    "    return fid_score\n",
    "\n",
    "def FID_measure(model, sample_size=100*8, batch_size=8, device=\"cpu\", real_path=\"dataset/cats/\", gen_path=\"generated_images\", epoch=int):\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Create a folder for generated images if it doesn't exist\n",
    "    os.makedirs(gen_path, exist_ok=True)\n",
    "    torch.manual_seed(2023)\n",
    "\n",
    "    # Generate images\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, sample_size, batch_size):\n",
    "            \n",
    "            # Random value from the normal distribution\n",
    "            z = torch.randn(batch_size, LATENT_DIM, 1, 1).to(device)\n",
    "\n",
    "            # Random from uniform distribution in range [-2, 2]\n",
    "            # z = torch.FloatTensor(batch_size, LATENT_DIM, 1, 1).uniform_(-2, 2).to(device)\n",
    "\n",
    "            # Generate images\n",
    "            gen_imgs = model(z)\n",
    "\n",
    "            # Save images\n",
    "            for j in range(batch_size):\n",
    "                save_image(gen_imgs[j], f\"{gen_path}/{i+j}.png\", normalize=True)\n",
    "        \n",
    "    # Measure FID score between the real and generated images\n",
    "    fid_score = run_fid(real_path, gen_path, epoch)\n",
    "    return fid_score\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load weight of model to measure FID score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weight( generator, discriminator=None, weight_path=\"\", index=int):\n",
    "\n",
    "    # Load the saved weights\n",
    "    generator.load_state_dict(torch.load(f'{weight_path}/G{index}.pt'))\n",
    "    # discriminator.load_state_dict(torch.load(f'{weight_path}/D{index}.pt'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from pathlib import Path\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(f'logs/cats/DCGAN/')\n",
    "\n",
    "# Specify the directory path\n",
    "directory_path = Path('weights/cats/DCGAN/')\n",
    "\n",
    "# Get the list of files ending with \".pth\"\n",
    "# file_list = list(directory_path.glob('*.pt'))\n",
    "\n",
    "\n",
    "gen = Generator(LATENT_DIM, CHANNELS_IMG, FEATURES_GEN).to(device)\n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    index_weight = i+1\n",
    "\n",
    "    load_weight(generator=gen, weight_path=directory_path, index=index_weight)\n",
    "\n",
    "    print('Epoch: ',index_weight)\n",
    "    fid_score = FID_measure(gen, sample_size, batch_size=16, device=f\"{device}\", gen_path=f\"gen_image/cats/DCGAN/{index_weight}/\", epoch=index_weight)   \n",
    "    \n",
    "    writer.add_scalar(\"Metrics/FID Score\", fid_score, index_weight)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
